{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Clone or pull the repo first\n",
        "%cd /content\n",
        "!test -d ADS509_FinalProject || git clone https://github.com/mitosisgg/ADS509_FinalProject.git\n",
        "%cd /content/ADS509_FinalProject\n",
        "!git pull"
      ],
      "metadata": {
        "id": "6EbwOUkUY1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json_to_csv_clean as j2c\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "xHC7QMUlcWe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# JSONs from the repo and write CSV back into the repo\n",
        "RAW_DIR  = Path(\"/content/ADS509_FinalProject/data/raw\")\n",
        "REPO_OUT = Path(\"/content/ADS509_FinalProject/articles.csv\")\n",
        "DRIVE_OUT = Path(\"/content/drive/MyDrive/Data/articles.csv\")\n",
        "\n",
        "# Regex helpers\n",
        "CAT_RX = re.compile(r\"([A-Za-z]+)_articles_\", re.IGNORECASE)\n",
        "TRUNC_RX = re.compile(r\"\\s*\\[\\+\\d+\\s+chars\\]\\s*$\")\n",
        "\n",
        "def pick_category(path: Path) -> str:\n",
        "    m = CAT_RX.search(path.name)\n",
        "    return (m.group(1) if m else \"unknown\").lower()\n",
        "\n",
        "def clean_content(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        return \"\"\n",
        "    return TRUNC_RX.sub(\"\", s)\n",
        "\n",
        "def load_items(path: Path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "    if isinstance(data, list):\n",
        "        return data\n",
        "    if isinstance(data, dict) and isinstance(data.get(\"articles\"), list):\n",
        "        return data[\"articles\"]\n",
        "    return []\n",
        "\n",
        "def main():\n",
        "    # Gather all JSON files\n",
        "    if not RAW_DIR.exists():\n",
        "        raise SystemExit(f\"Input folder not found: {RAW_DIR}\")\n",
        "\n",
        "    json_files = sorted(RAW_DIR.glob(\"*_articles_*.json\"))\n",
        "    if not json_files:\n",
        "        raise SystemExit(f\"No *_articles_*.json files found in {RAW_DIR}\")\n",
        "\n",
        "    frames = []\n",
        "\n",
        "    # Process each file\n",
        "    for fp in json_files:\n",
        "        recs = load_items(fp)\n",
        "        if not recs:\n",
        "            continue\n",
        "\n",
        "        df = pd.json_normalize(recs, sep=\".\")\n",
        "        out = pd.DataFrame({\n",
        "            \"category\":    [pick_category(fp)] * len(df),\n",
        "            \"title\":       df.get(\"title\"),\n",
        "            \"description\": df.get(\"description\"),\n",
        "            \"content\":     df.get(\"content\"),\n",
        "        })\n",
        "        cleaned = out[\"content\"].fillna(\"\").astype(str).map(clean_content)\n",
        "        out[\"content\"] = cleaned\n",
        "        out[\"content_len\"] = cleaned.str.len()\n",
        "        frames.append(out)\n",
        "\n",
        "    # Combine all DataFrames\n",
        "    if not frames:\n",
        "        raise SystemExit(\"All inputs parsed to empty; no rows to write.\")\n",
        "    combined = pd.concat(frames, ignore_index=True)\n",
        "\n",
        "    # Remove duplicates\n",
        "    combined = combined.drop_duplicates(subset=[\"title\", \"description\"], keep=\"first\")\n",
        "\n",
        "    # Save CSV to repo and Drive\n",
        "    REPO_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "    combined.to_csv(REPO_OUT, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    DRIVE_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "    combined.to_csv(DRIVE_OUT, index=False, encoding=\"utf-8\")\n",
        "\n",
        "    # Print summary + preview\n",
        "    print(f\"Wrote {REPO_OUT} (repo copy)\")\n",
        "    print(f\"Wrote {DRIVE_OUT} (Drive backup)\")\n",
        "    print(f\"Rows={len(combined)}  Cols={len(combined.columns)}\")\n",
        "    print(\"\\ncontent_len summary:\")\n",
        "    print(combined['content_len'].describe().to_string())\n",
        "    print(\"\\nPreview of first 5 rows:\")\n",
        "    print(combined[['category', 'title']].head(5).to_string(index=False))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Oum6MWc7clux"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}