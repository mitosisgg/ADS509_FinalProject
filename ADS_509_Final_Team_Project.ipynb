{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NewsAPI Classification and Text Analysis\n",
        "Authors: Christian Lee, Anahit Shekikyan, Graham Ward "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Library Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import html\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter, defaultdict\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function Creations\n",
        "\n",
        "Please use this section to develop functions or place them here from previous assignments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sw = stopwords.words(\"english\")\n",
        "\n",
        "punctuation = set(punctuation) # speeds up comparison\n",
        "\n",
        "# create compiled regex obj matching one or more whitespace\n",
        "ws_pattern = re.compile(r\"\\s+\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function creations\n",
        "\n",
        "# Functions from previous assignments before cell\n",
        "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
        "    \"\"\"\n",
        "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
        "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
        "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
        "        of unique tokens, lexical diversity, and number of characters. \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Fill in the correct values here. \n",
        "    total_tokens = len(tokens)\n",
        "    num_unique_tokens = len(set(tokens))\n",
        "    lexical_diversity = num_unique_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    num_characters = sum(len(token) for token in tokens)\n",
        "    \n",
        "    if verbose :        \n",
        "        print(f\"There are {total_tokens} tokens in the data.\")\n",
        "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
        "        print(f\"There are {num_characters} characters in the data.\")\n",
        "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
        "    \n",
        "        # print the five most common tokens\n",
        "        token_counts = Counter(tokens)\n",
        "        print(f\"The {num_tokens} most common tokens are:\")\n",
        "        for token, count in token_counts.most_common(num_tokens):\n",
        "            print(f\" {token}: {count}\")\n",
        "                    \n",
        "    return([total_tokens, num_unique_tokens,\n",
        "            lexical_diversity,\n",
        "            num_characters])\n",
        "\n",
        "# construct function for cleaning and tokenizing\n",
        "def clean_and_tokenize(text, stopwords):\n",
        "    \"\"\"\n",
        "    Clean and tokenize a single speech from the conventions.\n",
        "\n",
        "    This function applies the following steps:\n",
        "      1. Converts all characters to lowercase.\n",
        "      2. Removes punctuation characters by replacing them with spaces.\n",
        "      3. Collapses multiple whitespace characters into a single space and strips leading/trailing spaces.\n",
        "      4. Splits the cleaned string into tokens on whitespace.\n",
        "      5. Removes any tokens that are in the provided stopword set.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    text : str\n",
        "        A single text of a speech to clean and tokenize.\n",
        "    stopwords : set\n",
        "        A set of stopwords (all lowercase) to filter out from the resulting tokens.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of str\n",
        "        A list of cleaned tokens with punctuation removed, normalized case, and stopwords excluded.\n",
        "    \"\"\"\n",
        "    # handle coercion to string format error handling\n",
        "    if text is None:\n",
        "        return []\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    # make lowercase\n",
        "    text = text.lower()\n",
        "    # punctuation removal\n",
        "    text = \"\".join(ch if ch not in punctuation else \" \" for ch in text)\n",
        "    # include the removal of special characters\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    # replace whitespace with single whitespace and remove leading/trailing ws\n",
        "    text = ws_pattern.sub(\" \", text).strip()\n",
        "    # split cleaned string on space\n",
        "    tokens = text.split()\n",
        "    # remove stopwords\n",
        "    return [t for t in tokens if t not in stopwords]\n",
        "\n",
        "# special character locator and counter\n",
        "def find_special_characters(df, columns):\n",
        "    \"\"\"\n",
        "    Identify all unique special characters in one or more DataFrame columns,\n",
        "    and return their counts.\n",
        "    \n",
        "    Args:\n",
        "        df (pd.DataFrame): Input dataframe\n",
        "        columns (str or list): Column name(s) to search\n",
        "    \n",
        "    Returns:\n",
        "        dict: Dictionary of special characters and their counts\n",
        "    \"\"\"\n",
        "    if isinstance(columns, str):  # allow a single column\n",
        "        columns = [columns]\n",
        "        \n",
        "    pattern = re.compile(r\"[^a-zA-Z0-9\\s]\")\n",
        "    counter = Counter()\n",
        "    \n",
        "    for col in columns:\n",
        "        for text in df[col].dropna():\n",
        "            matches = pattern.findall(str(text))\n",
        "            counter.update(matches)\n",
        "    \n",
        "    return dict(counter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDu-CYTgbP9T"
      },
      "outputs": [],
      "source": [
        "# import the articles csv into a pandas dataframe for manipulation\n",
        "df = pd.read_csv('articles.csv')\n",
        "\n",
        "# print the head of the dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Descriptive Stats and Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print out the unique colummns of the dataframe\n",
        "print(df.columns)\n",
        "# print the data types of each column\n",
        "print(\"\\n\", df.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print the number of rows in the dataframe\n",
        "print(\"The number of rows in the dataframe are: \", df.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get counts for each category\n",
        "article_counts = df['category'].value_counts().reset_index()\n",
        "# counts\n",
        "article_counts.columns = ['category', 'count']\n",
        "# print\n",
        "print(article_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# visualization\n",
        "ax = sns.barplot(x='category', y='count', data=article_counts)\n",
        "\n",
        "ax.bar_label(ax.containers[0])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Category Counts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and get special character counts for contents\n",
        "content_special_characters = find_special_characters(df, [\"content\", \"description\", \"title\"])\n",
        "\n",
        "print(content_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# find and get special characters counts for description\n",
        "desc_special_characters = find_special_characters(df, [\"content\", \"description\", \"title\"])\n",
        "\n",
        "print(desc_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and get special character counts for titles\n",
        "title_special_characters = find_special_characters(df, [\"content\", \"description\", \"title\"])\n",
        "\n",
        "print(title_special_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identify any missing values in each column\n",
        "print(\"Missing values for each column\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# percentage\n",
        "print(\"\\nPercentage of missing per column: \")\n",
        "print((df.isnull().mean() * 100).round(2))\n",
        "\n",
        "# number of rows missing both description and content\n",
        "both_missing = df[df['description'].isnull() & df['content'].isnull()]\n",
        "print(f\"\\nThe number of observations missing BOTH description and content is: {len(both_missing)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# establish a new df_clean object for cleaning to keep original df\n",
        "df_clean = df.copy()\n",
        "\n",
        "# Establish regex patterns for removal and precleaning\n",
        "# html pattern\n",
        "html_tag_regex = re.compile(r\"<[^>]+>\")\n",
        "# removal of number of characters left in content\n",
        "truncate_tail_regex = re.compile(r\"\\[\\s*\\+\\s*\\d+\\s*chars?\\s*\\]\")\n",
        "# trailing ellipses at ends \n",
        "ellipsis_end_regex = re.compile(r\"(…|\\.{3,})\\s*$\")\n",
        "\n",
        "# text precleaning to remove \n",
        "def preclean_text(s: str) -> str:\n",
        "    \"\"\"\n",
        "    Pre-clean raw text from the NewsAPI dataset by stripping noise and formatting artifacts.\n",
        "\n",
        "    Steps performed:\n",
        "        1. Decode HTML entities (e.g., \"&amp;\" → \"&\").\n",
        "        2. Remove inline HTML tags (e.g., \"<p>\", \"<a href=...>\").\n",
        "        3. Strip NewsAPI truncation markers of the form \"[+123 chars]\".\n",
        "        4. Trim trailing ellipses at the end of text (\"...\", \"……\").\n",
        "        5. Collapse multiple whitespace characters into a single space and strip edges.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s : str\n",
        "        The input text string to be cleaned. Non-string or empty inputs are returned unchanged.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    str\n",
        "        A cleaned text string suitable for further cleaning.\n",
        "    \"\"\"\n",
        "    # handles null values\n",
        "    if not isinstance(s, str) or not s.strip():\n",
        "        return s\n",
        "    # convert html entries to normal text\n",
        "    x = html.unescape(s)\n",
        "    # remove HTML tags\n",
        "    x = html_tag_regex.sub(\" \", x)\n",
        "    # remove truncation tokens\n",
        "    x = truncate_tail_regex.sub(\" \", x)\n",
        "    # trim trailing ellipses\n",
        "    x = ellipsis_end_regex.sub(\" \", x)\n",
        "    # collapse whitespace and reduce leading and trailing ws\n",
        "    x = ws_pattern.sub(\" \", x).strip()\n",
        "    return x\n",
        "\n",
        "# apply precleaning function to the text columns\n",
        "for col in ['content', 'description', 'title']:\n",
        "    if col in df_clean.columns:\n",
        "        df_clean[col] = df_clean[col].apply(preclean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# apply the cleaning and tokenization to each column with text data.\n",
        "df_clean['title_tokens'] = df_clean['title'].apply(lambda x: clean_and_tokenize(x, sw))\n",
        "df_clean['desc_tokens'] = df_clean['description'].apply(lambda x: clean_and_tokenize(x, sw))\n",
        "df_clean['content_tokens'] = df_clean['content'].apply(lambda x: clean_and_tokenize(x, sw))\n",
        "\n",
        "# get some token counts\n",
        "df_clean['num_title_tokens'] = df_clean['title_tokens'].apply(len)\n",
        "df_clean['num_desc_tokens'] = df_clean['desc_tokens'].apply(len)\n",
        "df_clean['num_content_tokens'] = df_clean['content_tokens'].apply(len)\n",
        "\n",
        "# view head of clean data\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We still need to determine how we are going to handle the missing values for the description and content columns. Easiest recommendation is to just drop, we lose 10.5% of the content data, but can always rerun the older_fetch_articles python script to grab more data if needed as long as we build out this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the new shape of the dataframe after cleaning\n",
        "print(f\"The new dimensions of the data are : {df_clean.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# identify any missing values in each column\n",
        "print(\"Missing values for each column\")\n",
        "print(df_clean.isnull().sum())\n",
        "\n",
        "# percentage\n",
        "print(\"\\nPercentage of missing per column: \")\n",
        "print((df_clean.isnull().mean() * 100).round(2))\n",
        "\n",
        "# number of rows missing both description and content\n",
        "both_missing = df_clean[df_clean['description'].isnull() & df_clean['content'].isnull()]\n",
        "print(f\"\\nThe number of observations missing BOTH description and content is: {len(both_missing)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clean['comb_text'] = df_clean['title'] + ' ' + df_clean['description'] + ' ' + df_clean['content']\n",
        "df_clean['comb_tokens'] = df_clean['comb_text'].apply(lambda x: clean_and_tokenize(x, sw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build out wordclouds for the content_tokens of each category.\n",
        "# Config\n",
        "MAX_WORDS = 200\n",
        "MIN_TOKEN_LEN = 2\n",
        "# pixel size passed to WordCloud\n",
        "WIDTH, HEIGHT = 700, 300\n",
        "\n",
        "# get categories (sorted for consistent layout)\n",
        "categories = sorted(df_clean['category'].dropna().unique().tolist())\n",
        "\n",
        "# prepare subplots: 2 rows x 4 cols\n",
        "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
        "axes = axes.ravel()  # flatten to 1D index\n",
        "\n",
        "# for loop through the list of categories\n",
        "for i, cat in enumerate(categories):\n",
        "    # set axis for category\n",
        "    ax = axes[i]\n",
        "    # aggregate tokens for content_tokens category\n",
        "    tokens_series = df_clean.loc[df_clean['category'] == cat, 'comb_tokens']\n",
        "    freq = Counter()\n",
        "    for toks in tokens_series:\n",
        "        if isinstance(toks, list):\n",
        "            # light filter for readability\n",
        "            freq.update([t for t in toks if isinstance(t, str) and len(t) >= MIN_TOKEN_LEN])\n",
        "    # subplot formatting\n",
        "    ax.set_title(str(cat), fontsize=12, pad=8)\n",
        "    ax.axis(\"off\")\n",
        "    # handle the empty cases\n",
        "    if len(freq) == 0:\n",
        "        ax.text(0.5, 0.5, \"No tokens\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "        continue\n",
        "\n",
        "    # create the wordcloud itself\n",
        "    wc = WordCloud(\n",
        "        width=WIDTH,\n",
        "        height=HEIGHT,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"plasma\",\n",
        "        max_words=MAX_WORDS,\n",
        "        normalize_plurals=True\n",
        "    ).generate_from_frequencies(freq)\n",
        "\n",
        "    ax.imshow(wc, interpolation=\"bilinear\")\n",
        "\n",
        "# hide any unused axes\n",
        "for j in range(len(categories), 8):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Wordclouds of Content Tokens by Category\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build out wordclouds for the title_tokens of each category.\n",
        "# Config\n",
        "MAX_WORDS = 200\n",
        "MIN_TOKEN_LEN = 2\n",
        "# pixel size passed to WordCloud\n",
        "WIDTH, HEIGHT = 700, 300\n",
        "\n",
        "# get categories (sorted for consistent layout)\n",
        "categories = sorted(df_clean['category'].dropna().unique().tolist())\n",
        "\n",
        "# prepare subplots: 2 rows x 4 cols\n",
        "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
        "axes = axes.ravel()  # flatten to 1D index\n",
        "\n",
        "# for loop through the list of categories\n",
        "for i, cat in enumerate(categories):\n",
        "    # set axis for category\n",
        "    ax = axes[i]\n",
        "    # aggregate tokens for content_tokens category\n",
        "    tokens_series = df_clean.loc[df_clean['category'] == cat, 'title_tokens']\n",
        "    freq = Counter()\n",
        "    for toks in tokens_series:\n",
        "        if isinstance(toks, list):\n",
        "            # light filter for readability\n",
        "            freq.update([t for t in toks if isinstance(t, str) and len(t) >= MIN_TOKEN_LEN])\n",
        "    # subplot formatting\n",
        "    ax.set_title(str(cat), fontsize=12, pad=8)\n",
        "    ax.axis(\"off\")\n",
        "    # handle the empty cases\n",
        "    if len(freq) == 0:\n",
        "        ax.text(0.5, 0.5, \"No tokens\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "        continue\n",
        "\n",
        "    # create the wordcloud itself\n",
        "    wc = WordCloud(\n",
        "        width=WIDTH,\n",
        "        height=HEIGHT,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"plasma\",\n",
        "        max_words=MAX_WORDS,\n",
        "        normalize_plurals=True\n",
        "    ).generate_from_frequencies(freq)\n",
        "\n",
        "    ax.imshow(wc, interpolation=\"bilinear\")\n",
        "\n",
        "# hide any unused axes\n",
        "for j in range(len(categories), 8):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Wordclouds of Title Tokens by Category\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build out wordclouds for the desc_tokens of each category.\n",
        "# Config\n",
        "MAX_WORDS = 200\n",
        "MIN_TOKEN_LEN = 2\n",
        "# pixel size passed to WordCloud\n",
        "WIDTH, HEIGHT = 700, 300\n",
        "\n",
        "# get categories (sorted for consistent layout)\n",
        "categories = sorted(df_clean['category'].dropna().unique().tolist())\n",
        "\n",
        "# prepare subplots: 2 rows x 4 cols\n",
        "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
        "axes = axes.ravel()  # flatten to 1D index\n",
        "\n",
        "# for loop through the list of categories\n",
        "for i, cat in enumerate(categories):\n",
        "    # set axis for category\n",
        "    ax = axes[i]\n",
        "    # aggregate tokens for content_tokens category\n",
        "    tokens_series = df_clean.loc[df_clean['category'] == cat, 'desc_tokens']\n",
        "    freq = Counter()\n",
        "    for toks in tokens_series:\n",
        "        if isinstance(toks, list):\n",
        "            # light filter for readability\n",
        "            freq.update([t for t in toks if isinstance(t, str) and len(t) >= MIN_TOKEN_LEN])\n",
        "    # subplot formatting\n",
        "    ax.set_title(str(cat), fontsize=12, pad=8)\n",
        "    ax.axis(\"off\")\n",
        "    # handle the empty cases\n",
        "    if len(freq) == 0:\n",
        "        ax.text(0.5, 0.5, \"No tokens\", ha=\"center\", va=\"center\", fontsize=12)\n",
        "        continue\n",
        "\n",
        "    # create the wordcloud itself\n",
        "    wc = WordCloud(\n",
        "        width=WIDTH,\n",
        "        height=HEIGHT,\n",
        "        background_color=\"white\",\n",
        "        colormap=\"plasma\",\n",
        "        max_words=MAX_WORDS,\n",
        "        normalize_plurals=True\n",
        "    ).generate_from_frequencies(freq)\n",
        "\n",
        "    ax.imshow(wc, interpolation=\"bilinear\")\n",
        "\n",
        "# hide any unused axes\n",
        "for j in range(len(categories), 8):\n",
        "    axes[j].axis(\"off\")\n",
        "\n",
        "fig.suptitle(\"Wordclouds of Descriptive Tokens by Category\", fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use the descriptive stats for each _tokens column\n",
        "# identify the columns to use\n",
        "token_cols = ['title_tokens', 'desc_tokens', 'content_tokens']\n",
        "# create an empty dictionary\n",
        "desc_stats_summary = {}\n",
        "\n",
        "# for loop\n",
        "for col in token_cols:\n",
        "    # flatten column tokens into list\n",
        "    col_tokens = [t for tokens in df_clean[col] if isinstance(tokens, list) for t in tokens]\n",
        "    # apply desc stats function\n",
        "    stats = descriptive_stats(col_tokens, verbose=False)\n",
        "    # add results to the dictionary\n",
        "    desc_stats_summary[col] = {\n",
        "        \"Total Tokens\": stats[0],\n",
        "        \"Unique Tokens\": stats[1],\n",
        "        \"Lexical Diversity\": round(stats[2], 3),\n",
        "        \"Total Characters\": stats[3]\n",
        "    }\n",
        "\n",
        "# convert to df to view and transpose\n",
        "desc_stats_df = pd.DataFrame(desc_stats_summary).T\n",
        "display(desc_stats_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Splitting and Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# subset only the required columns and drop rows with missing values\n",
        "df_subset = df_clean[['category', 'comb_text']].dropna()\n",
        "\n",
        "# Perform stratified split to maintain class proportions\n",
        "train_df, test_df = train_test_split(\n",
        "    df_subset, \n",
        "    test_size=0.2, \n",
        "    random_state=42, \n",
        "    stratify=df_subset['category']  # This ensures proportional split\n",
        ")\n",
        "\n",
        "# Verify the proportions were maintained\n",
        "print(\"\\nCategory distribution in training set:\")\n",
        "print(train_df['category'].value_counts(normalize=True).round(3))\n",
        "print(\"\\nCategory distribution in test set:\")\n",
        "print(test_df['category'].value_counts(normalize=True).round(3))\n",
        "\n",
        "print(f\"\\nTrain shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Development (Training and Testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a pipeline: TF-IDF vectorizer + Logistic Regression\n",
        "logreg_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=10000, ngram_range=(1,2)),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "logreg_pipeline.fit(train_df['comb_text'], train_df['category'])\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = logreg_pipeline.predict(test_df['comb_text'])\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_df['category'], y_pred)\n",
        "\n",
        "# Get classification report as dict\n",
        "report = classification_report(test_df['category'], y_pred, output_dict=True)\n",
        "\n",
        "# Convert to results DataFrame\n",
        "classifier_results = pd.DataFrame(report).T\n",
        "classifier_results['accuracy'] = accuracy  # Add accuracy column for all rows\n",
        "\n",
        "# Display summary\n",
        "display(classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a pipeline: TF-IDF vectorizer + Linear SVM\n",
        "svm_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=10000, ngram_range=(1,2)),\n",
        "    LinearSVC(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "svm_pipeline.fit(train_df['comb_text'], train_df['category'])\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_svm = svm_pipeline.predict(test_df['comb_text'])\n",
        "\n",
        "# Calculate accuracy\n",
        "svm_accuracy = accuracy_score(test_df['category'], y_pred_svm)\n",
        "\n",
        "# Get classification report as dict\n",
        "svm_report = classification_report(test_df['category'], y_pred_svm, output_dict=True)\n",
        "\n",
        "# Convert to results DataFrame\n",
        "svm_classifier_results = pd.DataFrame(svm_report).T\n",
        "svm_classifier_results['accuracy'] = svm_accuracy  # Add accuracy column for all rows\n",
        "\n",
        "# Display summary\n",
        "display(svm_classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Create a pipeline: TF-IDF vectorizer + Multinomial Naive Bayes\n",
        "nb_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(max_features=10000, ngram_range=(1,2)),\n",
        "    MultinomialNB()\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "nb_pipeline.fit(train_df['comb_text'], train_df['category'])\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_nb = nb_pipeline.predict(test_df['comb_text'])\n",
        "\n",
        "# Calculate accuracy\n",
        "nb_accuracy = accuracy_score(test_df['category'], y_pred_nb)\n",
        "\n",
        "# Get classification report as dict\n",
        "nb_report = classification_report(test_df['category'], y_pred_nb, output_dict=True)\n",
        "\n",
        "# Convert to results DataFrame\n",
        "nb_classifier_results = pd.DataFrame(nb_report).T\n",
        "nb_classifier_results['accuracy'] = nb_accuracy  # Add accuracy column for all rows\n",
        "\n",
        "# Display summary\n",
        "display(nb_classifier_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Vectorize the text\n",
        "vectorizer = CountVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english'\n",
        ")\n",
        "X_train_counts = vectorizer.fit_transform(train_df['comb_text'])\n",
        "\n",
        "# Fit LDA model\n",
        "n_topics = 7  # You have 7 categories, so let's use 7 topics\n",
        "lda = LatentDirichletAllocation(\n",
        "    n_components=n_topics,\n",
        "    max_iter=10,\n",
        "    learning_method='online',\n",
        "    random_state=42\n",
        ")\n",
        "lda.fit(X_train_counts)\n",
        "\n",
        "# Display top words by topic\n",
        "def print_top_words(model, feature_names, n_top_words=10):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        top_features = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
        "        print(f\"Topic #{topic_idx + 1}: {', '.join(top_features)}\")\n",
        "\n",
        "print_top_words(lda, vectorizer.get_feature_names_out())\n",
        "\n",
        "# Assign each document to its most probable topic\n",
        "topic_assignments = lda.transform(X_train_counts).argmax(axis=1)\n",
        "train_df['lda_topic'] = topic_assignments\n",
        "\n",
        "### EVALUATION\n",
        "\n",
        "# Evaluate topic coherence with silhouette score\n",
        "try:\n",
        "    sil_score = silhouette_score(lda.transform(X_train_counts), topic_assignments)\n",
        "    print(f\"\\nSilhouette Score (topic separation): {sil_score:.3f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nSilhouette Score could not be computed: {e}\")\n",
        "\n",
        "# Show topic distribution counts\n",
        "topic_counts = train_df['lda_topic'].value_counts().sort_index()\n",
        "print(\"\\nDocument count per topic:\")\n",
        "print(topic_counts)\n",
        "\n",
        "# For each topic, find the most common category in train_df\n",
        "topic_to_category = (\n",
        "    train_df.groupby('lda_topic')['category']\n",
        "    .agg(lambda x: x.value_counts().idxmax())\n",
        "    .rename('most_common_category')\n",
        ")\n",
        "\n",
        "# Show the mapping of topic to most common category\n",
        "print(topic_to_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.9.6)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
